{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ff22ac-7cc7-4fdf-b8aa-16ed1dadf361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Sapiens-Pytorch-Inference'...\n",
      "remote: Enumerating objects: 188, done.\u001b[K\n",
      "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 188 (delta 6), reused 9 (delta 2), pack-reused 169 (from 1)\u001b[K\n",
      "Receiving objects: 100% (188/188), 48.73 MiB | 776.00 KiB/s, done.\n",
      "Resolving deltas: 100% (112/112), done.\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/ibaiGorordo/Sapiens-Pytorch-Inference.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dbe54e6-b5d8-4d6c-a9b7-e3048372fab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cyanos/Code/MPCS/Advanced_Data_Analytics/Sapiens-Pytorch-Inference\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pymc3 3.11.5 requires cachetools>=4.2.1, which is not installed.\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.26.4 which is incompatible.\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0dba78f-c2d8-40a2-bea3-adb26b274070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from typing import List\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    " \n",
    "from enum import Enum\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imread_from_url import imread_from_url\n",
    "from scipy.fftpack import dct, idct, fft, ifft\n",
    " \n",
    "from dataclasses import dataclass\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    " \n",
    "from huggingface_hub import hf_hub_download, hf_hub_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "389bef31-fb68-42d9-9812-8f6b4dcdd1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘models/sapiens_1b_goliath_best_goliath_mIoU_7994_epoch_151_torchscript.pt2’ already there; not retrieving.\n",
      "File ‘models/sapiens_1b_goliath_best_Sapiens-1B-Depth-Estimation-Single-Person-Inference-Results.jpggoliath_AP_640_torchscript.pt2’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "#NOTE!! THIS TAKES SEVERAL HOURS TO RUN, SO ONLY RUN IF DESIRED\n",
    "# !wget -nc https://huggingface.co/facebook/sapiens-seg-1b-torchscript/resolve/main/sapiens_1b_goliath_best_goliath_mIoU_7994_epoch_151_torchscript.pt2 -O models/sapiens_1b_goliath_best_goliath_mIoU_7994_epoch_151_torchscript.pt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aee103f-6dc4-49e9-b426-d712a6743a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskType(Enum):\n",
    "    DEPTH = \"depth\"\n",
    "    NORMAL = \"normal\"\n",
    "    SEG = \"seg\"\n",
    "    POSE = \"pose\"\n",
    " \n",
    " \n",
    "def download(url: str, filename: str):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            total = int(r.headers.get(\"content-length\", 0))\n",
    " \n",
    "            # tqdm has many interesting parameters. Feel free to experiment!\n",
    "            tqdm_params = {\n",
    "                \"total\": total,\n",
    "                \"miniters\": 1,\n",
    "                \"unit\": \"B\",\n",
    "                \"unit_scale\": True,\n",
    "                \"unit_divisor\": 1024,\n",
    "            }\n",
    "            with tqdm(**tqdm_params) as pb:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    pb.update(len(chunk))\n",
    "                    f.write(chunk)\n",
    " \n",
    " \n",
    "def download_hf_model(model_name: str, task_type: TaskType, model_dir: str = \"models\"):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    " \n",
    "    path = model_dir + \"/\" + model_name\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    " \n",
    "    print(f\"Model {model_name} not found, downloading from Hugging Face Hub...\")\n",
    " \n",
    "    model_version = \"_\".join(model_name.split(\"_\")[:2])\n",
    "    repo_id = \"facebook/sapiens\"\n",
    "    subdirectory = (\n",
    "        f\"sapiens_lite_host/torchscript/{task_type.value}/checkpoints/{model_version}\"\n",
    "    )\n",
    " \n",
    "    # hf_hub_download(repo_id=repo_id, filename=model_name, subfolder=subdirectory, local_dir=model_dir)\n",
    "    url = hf_hub_url(repo_id=repo_id, filename=model_name, subfolder=subdirectory)\n",
    "    download(url, path)\n",
    "    print(\"Model downloaded successfully to\", path)\n",
    " \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a39fe92-eea7-42b2-b5fc-d72599fe1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor(input_size: tuple[int, int],\n",
    "                        mean: List[float] = (0.485, 0.456, 0.406),\n",
    "                        std: List[float] = (0.229, 0.224, 0.225)):\n",
    "    return transforms.Compose([transforms.ToPILImage(),\n",
    "                               transforms.Resize(input_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=mean, std=std),\n",
    "                               transforms.Lambda(lambda x: x.unsqueeze(0))\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cf6c805-c419-43c2-8839-e85dacf38749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SapiensSegmentationType(Enum):\n",
    "    SEGMENTATION_1B = (\n",
    "        \"sapiens_1b_goliath_best_goliath_mIoU_7994_epoch_151_torchscript.pt2\"\n",
    "    )\n",
    " \n",
    " \n",
    "random = np.random.RandomState(11)\n",
    " \n",
    "# --------------------------------**** 28 Classes **** -------------------------------------------------\n",
    " \n",
    "classes = [\n",
    "    \"Background\",\n",
    "    \"Apparel\",\n",
    "    \"Face Neck\",\n",
    "    \"Hair\",\n",
    "    \"Left Foot\",\n",
    "    \"Left Hand\",\n",
    "    \"Left Lower Arm\",\n",
    "    \"Left Lower Leg\",\n",
    "    \"Left Shoe\",\n",
    "    \"Left Sock\",\n",
    "    \"Left Upper Arm\",\n",
    "    \"Left Upper Leg\",\n",
    "    \"Lower Clothing\",\n",
    "    \"Right Foot\",\n",
    "    \"Right Hand\",\n",
    "    \"Right Lower Arm\",\n",
    "    \"Right Lower Leg\",\n",
    "    \"Right Shoe\",\n",
    "    \"Right Sock\",\n",
    "    \"Right Upper Arm\",\n",
    "    \"Right Upper Leg\",\n",
    "    \"Torso\",\n",
    "    \"Upper Clothing\",\n",
    "    \"Lower Lip\",\n",
    "    \"Upper Lip\",\n",
    "    \"Lower Teeth\",\n",
    "    \"Upper Teeth\",\n",
    "    \"Tongue\",\n",
    "]\n",
    " \n",
    "colors = random.randint(0, 255, (len(classes) - 1, 3))\n",
    "colors = np.vstack((np.array([128, 128, 128]), colors)).astype(\n",
    "    np.uint8\n",
    ")  # Add background color\n",
    "colors = colors[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04a7543b-fe8e-4e2b-a56f-9e103bcc4376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SapiensSegmentation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        type: SapiensSegmentationType = SapiensSegmentationType.SEGMENTATION_1B,\n",
    "        device: torch.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        ),\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "    ):\n",
    "        path = download_hf_model(type.value, TaskType.SEG)\n",
    "        model = torch.jit.load(path)\n",
    "        model = model.eval()\n",
    "        self.model = model.to(device).to(dtype)\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.preprocessor = create_preprocessor(\n",
    "            input_size=(1024, 768)\n",
    "        )  # Only these values seem to work well\n",
    " \n",
    "    def __call__(self, img: np.ndarray) -> np.ndarray:\n",
    "        start = time.perf_counter()\n",
    " \n",
    "        # Model expects BGR, but we change to RGB here because the preprocessor will switch the channels also\n",
    "        input = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        tensor = self.preprocessor(input).to(self.device).to(self.dtype)\n",
    " \n",
    "        with torch.inference_mode():\n",
    "            results = self.model(tensor)\n",
    "        segmentation_map = postprocess_segmentation(results, img.shape[:2])\n",
    " \n",
    "        print(f\"Segmentation inference took: {time.perf_counter() - start:.4f} seconds\")\n",
    "        return segmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba5fa71b-a525-45b8-bb82-f1a56cedf4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_segmentation(results: torch.Tensor, img_shape: tuple[int, int]) -> np.ndarray:\n",
    "    result = results[0].cpu()\n",
    " \n",
    "    # Upsample the result to the original image size\n",
    "    logits = F.interpolate(result.unsqueeze(0), size=img_shape, mode=\"bilinear\").squeeze(0)\n",
    " \n",
    "    # Perform argmax to get the segmentation map\n",
    "    segmentation_map = logits.argmax(dim=0, keepdim=True)\n",
    " \n",
    "    # Covert to numpy array\n",
    "    segmentation_map = segmentation_map.float().numpy().squeeze()\n",
    " \n",
    "    return segmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bfb0b7c-c4fc-4780-9e49-2fe48a85b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('processed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b4200b49-eeb5-485b-abf6-6bbc736e3d06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation inference took: 105.3728 seconds\n",
      "Time taken: 105.3883 seconds\n",
      "Segmentation inference took: 110.9610 seconds\n",
      "Time taken: 110.9866 seconds\n",
      "Segmentation inference took: 110.1950 seconds\n",
      "Time taken: 110.2143 seconds\n",
      "Segmentation inference took: 115.4239 seconds\n",
      "Time taken: 115.4402 seconds\n",
      "Segmentation inference took: 120.9569 seconds\n",
      "Time taken: 120.9769 seconds\n",
      "Segmentation inference took: 113.1436 seconds\n",
      "Time taken: 113.1633 seconds\n",
      "Segmentation inference took: 109.7770 seconds\n",
      "Time taken: 109.7883 seconds\n",
      "Segmentation inference took: 105.8808 seconds\n",
      "Time taken: 105.8906 seconds\n",
      "Segmentation inference took: 105.7118 seconds\n",
      "Time taken: 105.7193 seconds\n",
      "Segmentation inference took: 108.3890 seconds\n",
      "Time taken: 108.4090 seconds\n",
      "Segmentation inference took: 109.7243 seconds\n",
      "Time taken: 109.7397 seconds\n",
      "Segmentation inference took: 107.5786 seconds\n",
      "Time taken: 107.5857 seconds\n",
      "Segmentation inference took: 108.6059 seconds\n",
      "Time taken: 108.6140 seconds\n",
      "Segmentation inference took: 109.9196 seconds\n",
      "Time taken: 109.9279 seconds\n",
      "Segmentation inference took: 108.0583 seconds\n",
      "Time taken: 108.0647 seconds\n",
      "Segmentation inference took: 109.5732 seconds\n",
      "Time taken: 109.5836 seconds\n",
      "Segmentation inference took: 112.6201 seconds\n",
      "Time taken: 112.6329 seconds\n",
      "Segmentation inference took: 110.6279 seconds\n",
      "Time taken: 110.6373 seconds\n",
      "Segmentation inference took: 130.0861 seconds\n",
      "Time taken: 130.1118 seconds\n",
      "Segmentation inference took: 125.6349 seconds\n",
      "Time taken: 125.6617 seconds\n",
      "Segmentation inference took: 116.3324 seconds\n",
      "Time taken: 116.3486 seconds\n",
      "Segmentation inference took: 117.5555 seconds\n",
      "Time taken: 117.5734 seconds\n",
      "Segmentation inference took: 116.4647 seconds\n",
      "Time taken: 116.4791 seconds\n",
      "Segmentation inference took: 114.2552 seconds\n",
      "Time taken: 114.2621 seconds\n",
      "Segmentation inference took: 114.2352 seconds\n",
      "Time taken: 114.2488 seconds\n",
      "Segmentation inference took: 114.3391 seconds\n",
      "Time taken: 114.3481 seconds\n",
      "Segmentation inference took: 118.6739 seconds\n",
      "Time taken: 118.6887 seconds\n",
      "Segmentation inference took: 149.8028 seconds\n",
      "Time taken: 149.8107 seconds\n",
      "Segmentation inference took: 132.7867 seconds\n",
      "Time taken: 132.7942 seconds\n",
      "Segmentation inference took: 106.2705 seconds\n",
      "Time taken: 106.2816 seconds\n"
     ]
    }
   ],
   "source": [
    "frames = [\"frame\"+str(x) for x in range(30)]\n",
    "for frame in frames:\n",
    "    \n",
    "    img_path = \"data/\"+frame + \".jpg\"\n",
    "    img = cv2.imread(img_path)\n",
    " \n",
    "    model_type = SapiensSegmentationType.SEGMENTATION_1B\n",
    "    estimator = SapiensSegmentation(model_type)\n",
    " \n",
    "    start = time.perf_counter()\n",
    "    segs = estimator(img)\n",
    "\n",
    "    mask = np.repeat((segs == 2)[:, :, np.newaxis], 3, axis = 2)\n",
    "    image_of_interest = img * mask\n",
    "\n",
    "    cv2.imwrite(\"processed_data/\" + frame + \".png\", image_of_interest)\n",
    " \n",
    "    # Free the GPU memory\n",
    "    del estimator  # Delete the model\n",
    "    torch.cuda.empty_cache()  # Clear the GPU cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db1506-c9ae-49d4-92d5-2c0a9a635ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
